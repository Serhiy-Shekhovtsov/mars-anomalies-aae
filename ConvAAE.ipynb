{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch CEA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import time\n",
    "import types\n",
    "import os\n",
    "from shutil import copyfile, copy\n",
    "\n",
    "# ML stuff\n",
    "import numpy as np\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# PyTorch stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary\n",
    "\n",
    "# custom utils\n",
    "from logger import TBLogger\n",
    "from extract_patches import *\n",
    "from pytorch_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_notebook_path = 'ConvAAE.ipynb'\n",
    "debug_vals = types.SimpleNamespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_current_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TINY = 1e-15\n",
    "\n",
    "patch_size = 64 # image size = 64 x 64 = 4096\n",
    "batch_size = 256\n",
    "emb_size = 3200\n",
    "conv_emb_size = 50 * 8 * 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ENCODER\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 64, 64]             160\n",
      "       BatchNorm2d-2           [-1, 16, 64, 64]              32\n",
      "         LeakyReLU-3           [-1, 16, 64, 64]               0\n",
      "            Conv2d-4           [-1, 16, 64, 64]           2,320\n",
      "       BatchNorm2d-5           [-1, 16, 64, 64]              32\n",
      "         LeakyReLU-6           [-1, 16, 64, 64]               0\n",
      "         MaxPool2d-7           [-1, 16, 32, 32]               0\n",
      "            Conv2d-8           [-1, 32, 32, 32]           4,640\n",
      "       BatchNorm2d-9           [-1, 32, 32, 32]              64\n",
      "        LeakyReLU-10           [-1, 32, 32, 32]               0\n",
      "           Conv2d-11           [-1, 32, 32, 32]           9,248\n",
      "      BatchNorm2d-12           [-1, 32, 32, 32]              64\n",
      "        LeakyReLU-13           [-1, 32, 32, 32]               0\n",
      "        MaxPool2d-14           [-1, 32, 16, 16]               0\n",
      "           Conv2d-15           [-1, 64, 16, 16]          18,496\n",
      "      BatchNorm2d-16           [-1, 64, 16, 16]             128\n",
      "        LeakyReLU-17           [-1, 64, 16, 16]               0\n",
      "           Conv2d-18           [-1, 64, 16, 16]          36,928\n",
      "      BatchNorm2d-19           [-1, 64, 16, 16]             128\n",
      "        LeakyReLU-20           [-1, 64, 16, 16]               0\n",
      "           Conv2d-21           [-1, 64, 16, 16]          36,928\n",
      "      BatchNorm2d-22           [-1, 64, 16, 16]             128\n",
      "        LeakyReLU-23           [-1, 64, 16, 16]               0\n",
      "        MaxPool2d-24             [-1, 64, 8, 8]               0\n",
      "           Conv2d-25             [-1, 50, 8, 8]          28,850\n",
      "      BatchNorm2d-26             [-1, 50, 8, 8]             100\n",
      "        LeakyReLU-27             [-1, 50, 8, 8]               0\n",
      "           Conv2d-28             [-1, 50, 8, 8]          22,550\n",
      "          Flatten-29                 [-1, 3200]               0\n",
      "================================================================\n",
      "Total params: 160,796\n",
      "Trainable params: 160,796\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 5.97\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 6.59\n",
      "----------------------------------------------------------------\n",
      "\n",
      "\n",
      "    DECODER\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Unflatten-1             [-1, 50, 8, 8]               0\n",
      "   ConvTranspose2d-2           [-1, 64, 16, 16]          51,264\n",
      "       BatchNorm2d-3           [-1, 64, 16, 16]             128\n",
      "         LeakyReLU-4           [-1, 64, 16, 16]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]          36,928\n",
      "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
      "         LeakyReLU-7           [-1, 64, 16, 16]               0\n",
      "            Conv2d-8           [-1, 64, 16, 16]          36,928\n",
      "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
      "        LeakyReLU-10           [-1, 64, 16, 16]               0\n",
      "  ConvTranspose2d-11           [-1, 32, 32, 32]          32,800\n",
      "      BatchNorm2d-12           [-1, 32, 32, 32]              64\n",
      "        LeakyReLU-13           [-1, 32, 32, 32]               0\n",
      "           Conv2d-14           [-1, 32, 32, 32]           9,248\n",
      "      BatchNorm2d-15           [-1, 32, 32, 32]              64\n",
      "        LeakyReLU-16           [-1, 32, 32, 32]               0\n",
      "           Conv2d-17           [-1, 32, 32, 32]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 32, 32]              64\n",
      "        LeakyReLU-19           [-1, 32, 32, 32]               0\n",
      "  ConvTranspose2d-20           [-1, 16, 64, 64]           8,208\n",
      "      BatchNorm2d-21           [-1, 16, 64, 64]              32\n",
      "        LeakyReLU-22           [-1, 16, 64, 64]               0\n",
      "           Conv2d-23           [-1, 16, 64, 64]           2,320\n",
      "      BatchNorm2d-24           [-1, 16, 64, 64]              32\n",
      "        LeakyReLU-25           [-1, 16, 64, 64]               0\n",
      "           Conv2d-26            [-1, 1, 64, 64]             145\n",
      "================================================================\n",
      "Total params: 187,729\n",
      "Trainable params: 187,729\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.43\n",
      "Params size (MB): 0.72\n",
      "Estimated Total Size (MB): 7.16\n",
      "----------------------------------------------------------------\n",
      "\n",
      "\n",
      "    DISCRIMINATOR\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 30]          96,030\n",
      "         LeakyReLU-2                [-1, 1, 30]               0\n",
      "            Linear-3                [-1, 1, 10]             310\n",
      "         LeakyReLU-4                [-1, 1, 10]               0\n",
      "            Linear-5                 [-1, 1, 1]              11\n",
      "           Sigmoid-6                 [-1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 96,351\n",
      "Trainable params: 96,351\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.37\n",
      "Estimated Total Size (MB): 0.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# just a namespace\n",
    "class ConvAAE():\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, image_channels=1):\n",
    "            super(ConvAAE.Encoder, self).__init__()\n",
    "            \n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(image_channels, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                \n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                \n",
    "                nn.Conv2d(64, 50, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(50),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(50, 50, kernel_size=3, stride=1, padding=1),\n",
    "#                 nn.BatchNorm2d(50),\n",
    "#                 nn.LeakyReLU(),\n",
    "                \n",
    "#                 nn.Conv2d(50, 50, kernel_size=3, stride=1, padding=1),\n",
    "#                 nn.BatchNorm2d(50),\n",
    "#                 nn.LeakyReLU(),\n",
    "                \n",
    "#                 nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "                Flatten()\n",
    "            )\n",
    "\n",
    "        def forward(self, input_data):\n",
    "            # generate embeddings for our images\n",
    "            embeddings = self.encoder(input_data)\n",
    "\n",
    "            return embeddings\n",
    "\n",
    "        \n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, image_channels=1):\n",
    "            super(ConvAAE.Decoder, self).__init__()\n",
    "            \n",
    "            self.decoder = nn.Sequential(\n",
    "                Unflatten(C=50, H=8, W=8),\n",
    "\n",
    "                # upsampling + conv\n",
    "                nn.ConvTranspose2d(50, 64, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(64, 64, 3, 1, 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(64, 64, 3, 1, 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "\n",
    "                # upsampling + conv\n",
    "                nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(),\n",
    "\n",
    "                # upsampling + conv\n",
    "                nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Conv2d(16, 16, 3, 1, 1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "#                 # upsampling + conv\n",
    "#                 nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2, padding=1),\n",
    "#                 nn.BatchNorm2d(16),\n",
    "#                 nn.LeakyReLU(),\n",
    "                \n",
    "#                 nn.Conv2d(16, 16, 3, 1, 1),\n",
    "#                 nn.BatchNorm2d(16),\n",
    "#                 nn.LeakyReLU(),\n",
    "\n",
    "                nn.Conv2d(16, image_channels, 3, 1, 1),\n",
    "            )\n",
    "\n",
    "\n",
    "        def forward(self, input_embeddings):\n",
    "            data = self.decoder(input_embeddings)\n",
    "            \n",
    "            return data\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, image_channels=1):\n",
    "            super(ConvAAE.Discriminator, self).__init__()\n",
    "\n",
    "            self.discriminator = nn.Sequential(\n",
    "                nn.Linear(emb_size, 30),\n",
    "                nn.LeakyReLU(),\n",
    "                \n",
    "                nn.Linear(30, 10),\n",
    "                nn.LeakyReLU(),\n",
    "\n",
    "                # linear layer with 1 element\n",
    "                nn.Linear(10, 1),\n",
    "\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, input_embeddings):\n",
    "            # process embeddings and guess if it's real or fake\n",
    "            discriminator_output = self.discriminator(input_embeddings)\n",
    "\n",
    "            return discriminator_output\n",
    "\n",
    "print('    ENCODER')\n",
    "summary(ConvAAE.Encoder().to(device), (1, patch_size, patch_size))\n",
    "\n",
    "print('\\n\\n    DECODER')\n",
    "summary(ConvAAE.Decoder().to(device), (1, emb_size))\n",
    "\n",
    "print('\\n\\n    DISCRIMINATOR')\n",
    "summary(ConvAAE.Discriminator().to(device), (1, emb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "\n",
    "\n",
    "def reset_model():\n",
    "    global get_reconstruction_loss, encoder_net, decoder_net, discrim_net\n",
    "    global decoder_optimizer, encoder_optimizer, encoder_generator_optimizer, discriminator_optimizer\n",
    "\n",
    "    get_reconstruction_loss = nn.MSELoss()\n",
    "\n",
    "    # creating sub-models\n",
    "    encoder_net = ConvAAE.Encoder().to(device)\n",
    "    decoder_net = ConvAAE.Decoder().to(device)\n",
    "    discrim_net = ConvAAE.Discriminator().to(device)\n",
    "\n",
    "    # optimizes decoder\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_net.parameters(), weight_decay=0, lr=learning_rate)\n",
    "\n",
    "    # optimizes encoder/generator\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_net.parameters(), weight_decay=0, lr=learning_rate)\n",
    "\n",
    "    # optimizes encoder/generator\n",
    "    encoder_generator_optimizer = torch.optim.Adam(encoder_net.parameters(), \n",
    "        weight_decay=0, lr=learning_rate / 100)\n",
    "\n",
    "    # optimizes discriminator\n",
    "    discriminator_optimizer = torch.optim.Adam(discrim_net.parameters(), \n",
    "        weight_decay=0, lr=learning_rate / 50)\n",
    "\n",
    "\n",
    "reset_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "files_per_epoch = 5\n",
    "\n",
    "# get all red files\n",
    "red_img_files = [f for f in listdir(data_path)\n",
    "                 if isfile(join(data_path, f)) and 'RED' in f]\n",
    "\n",
    "# random shuffle\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(red_img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def get_new_epoch_patches():\n",
    "    np.random.shuffle(red_img_files)\n",
    "    # list of .IMG files names\n",
    "    epoch_files = red_img_files[:files_per_epoch]\n",
    "\n",
    "    # extract patches and concatenate all of them into one list\n",
    "    images, ids = extract_patches_from_img(epoch_files, patch_size=patch_size)\n",
    "    \n",
    "    images, ids = shuffle(images, ids)\n",
    "    \n",
    "    # numpy to tensor\n",
    "    tensor_images = numpy_images_to_tensor_dataset(images)\n",
    "    \n",
    "    tensor_images = tensor_images / 255\n",
    "    \n",
    "    # create loader for PyTorch\n",
    "    dataset = torch.utils.data.TensorDataset(tensor_images)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(log_name, print_interval=20):\n",
    "    global log_images\n",
    "    \n",
    "    # images used for logging and testing\n",
    "    log_images = None\n",
    "    \n",
    "    log_dir = './logs/' + log_name\n",
    "    \n",
    "    # make sure we are not training in existing directory\n",
    "    if os.path.isdir(log_dir):\n",
    "        raise ValueError('Please, use a new logging directory for training')\n",
    "        \n",
    "    if not os.path.exists(log_dir + '/models'):\n",
    "        os.makedirs(log_dir + '/models')\n",
    "\n",
    "    # make a new logger for new directory\n",
    "    tb_logger = TBLogger(log_dir)\n",
    "    \n",
    "    # make a copy so you we can recall what/how exactly we trained this time\n",
    "    copy(current_notebook_path, log_dir)\n",
    "    \n",
    "    # now we can start the training\n",
    "    encoder_net.train(), decoder_net.train() #, discrim_net.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        reconstruction_losses = []\n",
    "        generator_losses = []\n",
    "        discriminator_losses = []\n",
    "        \n",
    "        # get random group of images\n",
    "        # split them into patches, combine to one list\n",
    "        # and convert this list into PyTorch dataset\n",
    "        loader = get_new_epoch_patches()\n",
    "        \n",
    "        # loop over mini-batches sampled from patches in current dataset\n",
    "        for i, (data,) in enumerate(loader):\n",
    "            \n",
    "            # store some images to track the training progress\n",
    "            if log_images is None:\n",
    "                log_images = data.clone().to(device)\n",
    "            \n",
    "            images = data.to(device)\n",
    "            \n",
    "            ### FORWARD PASS THROUGH TWO OUTPUTS\n",
    "            \n",
    "            ## Autoencoder part\n",
    "            # run through encoder net and get embeddings\n",
    "            embeddings = encoder_net(images)\n",
    "\n",
    "            # run embeddings through dencoder net and get reconstruction\n",
    "            reconstructed_images = decoder_net(embeddings)\n",
    "\n",
    "            reconstruction_loss = get_reconstruction_loss(images, reconstructed_images)\n",
    "            \n",
    "            ## Generator + Discriminator part\n",
    "            \n",
    "            # they are fake, but it's \"normal\" :)\n",
    "            batch_size = images.size()[0]\n",
    "            fake_embeddings = Variable(torch.randn(batch_size, emb_size) * 5.).to(device)\n",
    "\n",
    "            real_embeddings_discrimination = discrim_net(embeddings)\n",
    "\n",
    "            # discriminator loss\n",
    "            discriminator_loss = -torch.mean(torch.log(discrim_net(fake_embeddings) + TINY) + \n",
    "                                             torch.log(1 - real_embeddings_discrimination + TINY))\n",
    "\n",
    "            # we are trying to maximize generator's loss on real embeddings\n",
    "            generator_loss = -torch.mean(torch.log(real_embeddings_discrimination + TINY))\n",
    "\n",
    "            ### BACKWARD PASS\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            # update gradients of encoder + decoder\n",
    "            reconstruction_loss.backward(retain_graph=True)\n",
    "\n",
    "            # update encoder and decoder models\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            # update gradients of encoder as generator\n",
    "            \n",
    "            encoder_generator_optimizer.zero_grad()\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            encoder_generator_optimizer.step()\n",
    "\n",
    "            # update gradients of discriminator only\n",
    "            # reset negative generator gradients\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            reconstruction_losses.append(reconstruction_loss.item())\n",
    "            generator_losses.append(generator_loss.item())\n",
    "            discriminator_losses.append(discriminator_loss.item())\n",
    "            \n",
    "        \n",
    "        # get mean averaged per mini-batch\n",
    "        mean_r_loss = np.mean(reconstruction_losses)\n",
    "        mean_g_loss = np.mean(generator_losses)\n",
    "        mean_d_loss = np.mean(discriminator_losses)\n",
    "        \n",
    "        # log to file on every epoch\n",
    "        tb_logger.add_loss(mean_r_loss)\n",
    "        tb_logger.log_scalar('generator/loss', mean_g_loss)\n",
    "        tb_logger.log_scalar('discriminator/loss', mean_d_loss)\n",
    "        \n",
    "        # calculate and log val loss\n",
    "        reconstructed_images = decoder_net(encoder_net(log_images))\n",
    "        reconstruction_val_loss = get_reconstruction_loss(log_images, reconstructed_images)\n",
    "        tb_logger.log_scalar('validation/loss', reconstruction_val_loss.item())\n",
    "        \n",
    "        if epoch == 0:\n",
    "            # print original raw image\n",
    "            save_img_grid(f'{log_dir}/_src_image.jpg', encoder_net, decoder_net, log_images, print_input=True)\n",
    "        \n",
    "        # log to console from time to time\n",
    "        if epoch % print_interval == 0:\n",
    "            print(time.strftime('%X') + ' - epoch [{}/{}], r loss: {:.8f}, g loss: {:.4f}, d loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, mean_r_loss, mean_g_loss, mean_d_loss))\n",
    "            \n",
    "            # save the model for myself and model state for sharing with others\n",
    "            torch.save(encoder_net.state_dict(), f'{log_dir}/models/encoder_net_e_{epoch}.pth')\n",
    "            torch.save(decoder_net.state_dict(), f'{log_dir}/models/decoder_net_e_{epoch}.pth')\n",
    "            torch.save(discrim_net.state_dict(), f'{log_dir}/models/discrim_net_e_{epoch}.pth')\n",
    "            \n",
    "            save_img_grid(f'{log_dir}/e_{epoch}_image.jpg', encoder_net, decoder_net, log_images)\n",
    "            \n",
    "            # save the histogram. time consuming operation\n",
    "            tb_logger.log_histogram('histograms/encoder', embeddings.clone().detach().cpu())\n",
    "    \n",
    "    # saving the final model\n",
    "    torch.save(encoder_net.state_dict(), f'{log_dir}/models/encoder_net_e_{epoch}.pth')\n",
    "    torch.save(decoder_net.state_dict(), f'{log_dir}/models/decoder_net_e_{epoch}.pth')\n",
    "    torch.save(discrim_net.state_dict(), f'{log_dir}/models/discrim_net_e_{epoch}.pth')\n",
    "    \n",
    "    print('DONE')\n",
    "\n",
    "def save_test_img(fname, encoder_net, decoder_net, tensor_images, print_input=False):\n",
    "    test_data = tensor_images[:1]\n",
    "\n",
    "    # forward\n",
    "    if print_input:\n",
    "        np_output = test_data.data.cpu().numpy()\n",
    "    else:\n",
    "        test_output = encoder_net(test_data)\n",
    "        test_output = decoder_net(test_output)\n",
    "        np_output = test_output.data.cpu().numpy()\n",
    "    \n",
    "    plt.imsave(fname, np.squeeze(np_output), vmin=test_data.min(), vmax=test_data.max())\n",
    "    \n",
    "def save_img_grid(fname, encoder_net, decoder_net, tensor_images, print_input=False):\n",
    "    grid_size = 8\n",
    "    \n",
    "    test_data = tensor_images[:grid_size ** 2]\n",
    "    \n",
    "    # forward\n",
    "    if print_input:\n",
    "        np_output = test_data.data.cpu().numpy()\n",
    "    else:\n",
    "        test_output = encoder_net(test_data)\n",
    "        test_output = decoder_net(test_output)\n",
    "        np_output = test_output.data.cpu().numpy()\n",
    "    \n",
    "    fig, img_plots = plt.subplots(grid_size, grid_size, figsize=(8, 8), gridspec_kw = {'wspace':0.01, 'hspace':0.01})\n",
    "\n",
    "    fig.patch.set_facecolor('black')\n",
    "\n",
    "    for i in range(0, grid_size):\n",
    "        for j in range(0, grid_size):\n",
    "            img_plt = img_plots[i, j]\n",
    "\n",
    "            img = np_output[i * grid_size + j]\n",
    "            img_plt.imshow(np.squeeze(img), vmin=test_data.min(), vmax=test_data.max())\n",
    "            img_plt.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "    \n",
    "    fig.savefig(fname)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_images.min(), log_images.max(), log_images.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Please, use new logging directory name  before each training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:22:15 - epoch [1/1500], r loss: 0.32265964, g loss: 0.5614, d loss: 1.3526\n",
      "20:36:32 - epoch [6/1500], r loss: 0.07376406, g loss: 0.5760, d loss: 1.3288\n",
      "20:57:18 - epoch [11/1500], r loss: 0.00337090, g loss: 0.5896, d loss: 1.3053\n",
      "21:11:51 - epoch [16/1500], r loss: 0.00112747, g loss: 0.5979, d loss: 1.2902\n",
      "21:29:17 - epoch [21/1500], r loss: 0.00072740, g loss: 0.6107, d loss: 1.2701\n",
      "21:45:29 - epoch [26/1500], r loss: 0.00058217, g loss: 0.6191, d loss: 1.2557\n",
      "22:03:44 - epoch [31/1500], r loss: 0.00056214, g loss: 0.6376, d loss: 1.2289\n",
      "22:20:39 - epoch [36/1500], r loss: 0.00058928, g loss: 0.6389, d loss: 1.2226\n",
      "22:39:31 - epoch [41/1500], r loss: 0.00015694, g loss: 0.6457, d loss: 1.2102\n",
      "22:56:12 - epoch [46/1500], r loss: 0.00010201, g loss: 0.6555, d loss: 1.1965\n",
      "23:12:15 - epoch [51/1500], r loss: 0.00016270, g loss: 0.6685, d loss: 1.1778\n",
      "23:31:12 - epoch [56/1500], r loss: 0.00038326, g loss: 0.6752, d loss: 1.1671\n",
      "23:49:50 - epoch [61/1500], r loss: 0.00031130, g loss: 0.6835, d loss: 1.1554\n",
      "00:06:32 - epoch [66/1500], r loss: 0.00012654, g loss: 0.6970, d loss: 1.1349\n",
      "00:24:21 - epoch [71/1500], r loss: 0.00020616, g loss: 0.7173, d loss: 1.1099\n",
      "00:41:39 - epoch [76/1500], r loss: 0.00031378, g loss: 0.7079, d loss: 1.1175\n",
      "01:00:40 - epoch [81/1500], r loss: 0.00007878, g loss: 0.7308, d loss: 1.0918\n",
      "01:16:02 - epoch [86/1500], r loss: 0.00036955, g loss: 0.7327, d loss: 1.0873\n",
      "01:32:21 - epoch [91/1500], r loss: 0.00047600, g loss: 0.7742, d loss: 1.0427\n",
      "01:51:13 - epoch [96/1500], r loss: 0.00018990, g loss: 0.7641, d loss: 1.0504\n",
      "02:09:40 - epoch [101/1500], r loss: 0.00008992, g loss: 0.7620, d loss: 1.0494\n",
      "02:29:24 - epoch [106/1500], r loss: 0.00051779, g loss: 0.7626, d loss: 1.0474\n",
      "02:48:46 - epoch [111/1500], r loss: 0.00030844, g loss: 0.8245, d loss: 0.9848\n",
      "03:06:04 - epoch [116/1500], r loss: 0.00011628, g loss: 0.8042, d loss: 0.9992\n",
      "03:23:51 - epoch [121/1500], r loss: 0.00007420, g loss: 0.7913, d loss: 1.0084\n",
      "03:42:20 - epoch [126/1500], r loss: 0.00020216, g loss: 0.8454, d loss: 0.9572\n",
      "04:00:53 - epoch [131/1500], r loss: 0.00011059, g loss: 0.8471, d loss: 0.9528\n",
      "04:18:27 - epoch [136/1500], r loss: 0.00034946, g loss: 0.8461, d loss: 0.9488\n",
      "04:36:48 - epoch [141/1500], r loss: 0.00019700, g loss: 0.8524, d loss: 0.9405\n",
      "04:52:55 - epoch [146/1500], r loss: 0.00010724, g loss: 0.8592, d loss: 0.9322\n",
      "05:13:18 - epoch [151/1500], r loss: 0.00016112, g loss: 0.8583, d loss: 0.9307\n",
      "05:30:54 - epoch [156/1500], r loss: 0.00010041, g loss: 0.8674, d loss: 0.9200\n",
      "05:47:27 - epoch [161/1500], r loss: 0.00018569, g loss: 0.8679, d loss: 0.9169\n",
      "06:05:07 - epoch [166/1500], r loss: 0.00009875, g loss: 0.8880, d loss: 0.8987\n"
     ]
    }
   ],
   "source": [
    "reset_model()\n",
    "\n",
    "num_epochs = 1500\n",
    "\n",
    "# specify a new name for log directory!\n",
    "train_model('added adversarial component ' + time.strftime('%H-%M-%S'), print_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:08:47 - epoch [1/1500], r loss: 0.00005139, g loss: 0.6922, d loss: 1.4305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-93be231b72a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# specify a new name for log directory!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'added adversarial component '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%H-%M-%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-2c45f8e802a8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(log_name, print_interval)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# reset negative generator gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1500\n",
    "\n",
    "# specify a new name for log directory!\n",
    "train_model('added adversarial component ' + time.strftime('%H-%M-%S'), print_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:07:00 - epoch [1/2000], loss:7170.91958926\n",
      "01:08:04 - epoch [6/2000], loss:88.76065694\n",
      "01:09:07 - epoch [11/2000], loss:35.16238949\n",
      "01:10:11 - epoch [16/2000], loss:9.74772633\n",
      "01:11:14 - epoch [21/2000], loss:19.80191330\n",
      "01:12:18 - epoch [26/2000], loss:5.94075181\n",
      "01:13:20 - epoch [31/2000], loss:17.81406898\n",
      "01:14:24 - epoch [36/2000], loss:5.08296149\n",
      "01:15:27 - epoch [41/2000], loss:17.50005694\n",
      "01:16:31 - epoch [46/2000], loss:4.69269189\n",
      "01:17:34 - epoch [51/2000], loss:17.45890252\n",
      "01:18:38 - epoch [56/2000], loss:4.49350875\n",
      "01:19:40 - epoch [61/2000], loss:16.39188738\n",
      "01:20:44 - epoch [66/2000], loss:4.42343875\n",
      "01:21:46 - epoch [71/2000], loss:16.17326309\n",
      "01:22:50 - epoch [76/2000], loss:4.28442504\n",
      "01:23:52 - epoch [81/2000], loss:15.79591764\n",
      "01:24:56 - epoch [86/2000], loss:4.19751638\n",
      "01:25:59 - epoch [91/2000], loss:15.42403963\n",
      "01:27:03 - epoch [96/2000], loss:4.28876181\n",
      "01:28:05 - epoch [101/2000], loss:15.70192102\n",
      "01:29:09 - epoch [106/2000], loss:4.16870721\n",
      "01:30:12 - epoch [111/2000], loss:15.50390015\n",
      "01:31:16 - epoch [116/2000], loss:4.08238714\n",
      "01:32:19 - epoch [121/2000], loss:14.56241238\n",
      "01:33:24 - epoch [126/2000], loss:3.95523038\n",
      "01:34:27 - epoch [131/2000], loss:14.36809412\n",
      "01:35:31 - epoch [136/2000], loss:3.90610255\n",
      "01:36:33 - epoch [141/2000], loss:14.18905124\n",
      "01:37:37 - epoch [146/2000], loss:3.86344271\n",
      "01:38:40 - epoch [151/2000], loss:14.01105573\n",
      "01:39:43 - epoch [156/2000], loss:3.82115455\n",
      "01:40:45 - epoch [161/2000], loss:13.98038134\n",
      "01:41:49 - epoch [166/2000], loss:3.78073727\n",
      "01:42:52 - epoch [171/2000], loss:13.67445035\n",
      "01:43:55 - epoch [176/2000], loss:3.80093145\n",
      "01:44:58 - epoch [181/2000], loss:13.61685250\n",
      "01:46:02 - epoch [186/2000], loss:3.71543480\n",
      "01:47:04 - epoch [191/2000], loss:13.43403879\n",
      "01:48:08 - epoch [196/2000], loss:3.81772208\n",
      "01:49:11 - epoch [201/2000], loss:13.34618842\n",
      "01:50:14 - epoch [206/2000], loss:3.65626543\n",
      "01:51:17 - epoch [211/2000], loss:13.18392639\n",
      "01:52:21 - epoch [216/2000], loss:3.69876656\n",
      "01:53:23 - epoch [221/2000], loss:13.12029452\n",
      "01:54:27 - epoch [226/2000], loss:3.59066332\n",
      "01:55:29 - epoch [231/2000], loss:12.92656615\n",
      "01:56:33 - epoch [236/2000], loss:3.54753205\n",
      "01:57:36 - epoch [241/2000], loss:12.63638946\n",
      "01:58:40 - epoch [246/2000], loss:3.51384051\n",
      "01:59:42 - epoch [251/2000], loss:12.39203577\n",
      "02:00:46 - epoch [256/2000], loss:3.45879292\n",
      "02:01:49 - epoch [261/2000], loss:12.10937971\n",
      "02:02:53 - epoch [266/2000], loss:3.42196142\n",
      "02:03:56 - epoch [271/2000], loss:11.80427058\n",
      "02:04:59 - epoch [276/2000], loss:3.38895636\n",
      "02:06:02 - epoch [281/2000], loss:11.53721868\n",
      "02:07:06 - epoch [286/2000], loss:3.39782464\n",
      "02:08:08 - epoch [291/2000], loss:11.26625884\n",
      "02:09:12 - epoch [296/2000], loss:3.33443274\n",
      "02:10:15 - epoch [301/2000], loss:10.97433429\n",
      "02:11:19 - epoch [306/2000], loss:3.30765564\n",
      "02:12:21 - epoch [311/2000], loss:10.65604280\n",
      "02:13:25 - epoch [316/2000], loss:3.28171367\n",
      "02:14:28 - epoch [321/2000], loss:10.39396685\n",
      "02:15:32 - epoch [326/2000], loss:3.25608395\n",
      "02:16:35 - epoch [331/2000], loss:10.12496585\n",
      "02:17:39 - epoch [336/2000], loss:3.23306475\n",
      "02:18:41 - epoch [341/2000], loss:9.91468400\n",
      "02:19:45 - epoch [346/2000], loss:3.21449750\n",
      "02:20:48 - epoch [351/2000], loss:9.73113953\n",
      "02:21:51 - epoch [356/2000], loss:3.19174497\n",
      "02:22:54 - epoch [361/2000], loss:9.97593551\n",
      "02:23:58 - epoch [366/2000], loss:3.16793984\n",
      "02:25:00 - epoch [371/2000], loss:9.34668733\n",
      "02:26:04 - epoch [376/2000], loss:3.14382725\n",
      "02:27:06 - epoch [381/2000], loss:9.17978811\n",
      "02:28:10 - epoch [386/2000], loss:3.12590296\n",
      "02:29:12 - epoch [391/2000], loss:9.07067237\n",
      "02:30:16 - epoch [396/2000], loss:3.10671151\n",
      "02:31:18 - epoch [401/2000], loss:8.89475439\n",
      "02:32:21 - epoch [406/2000], loss:3.08668831\n",
      "02:33:24 - epoch [411/2000], loss:8.75827710\n",
      "02:34:29 - epoch [416/2000], loss:3.06876726\n",
      "02:35:31 - epoch [421/2000], loss:8.76390007\n",
      "02:36:35 - epoch [426/2000], loss:3.05227758\n",
      "02:37:37 - epoch [431/2000], loss:8.51933173\n",
      "02:38:40 - epoch [436/2000], loss:3.03415547\n",
      "02:39:43 - epoch [441/2000], loss:8.42591297\n",
      "02:40:47 - epoch [446/2000], loss:3.01969638\n",
      "02:41:49 - epoch [451/2000], loss:8.37824995\n",
      "02:42:53 - epoch [456/2000], loss:3.00653390\n",
      "02:43:56 - epoch [461/2000], loss:8.27856485\n",
      "02:44:59 - epoch [466/2000], loss:2.99432373\n",
      "02:46:02 - epoch [471/2000], loss:8.75484468\n",
      "02:47:06 - epoch [476/2000], loss:3.10096049\n",
      "02:48:08 - epoch [481/2000], loss:8.70816883\n",
      "02:49:12 - epoch [486/2000], loss:3.04085480\n",
      "02:50:14 - epoch [491/2000], loss:8.07445792\n",
      "02:51:18 - epoch [496/2000], loss:2.96487160\n",
      "02:52:20 - epoch [501/2000], loss:8.04314231\n",
      "02:53:24 - epoch [506/2000], loss:2.95611976\n",
      "02:54:26 - epoch [511/2000], loss:8.61239185\n",
      "02:55:30 - epoch [516/2000], loss:2.95679899\n",
      "02:56:33 - epoch [521/2000], loss:7.91168987\n",
      "02:57:36 - epoch [526/2000], loss:2.94097355\n",
      "02:58:38 - epoch [531/2000], loss:7.88625075\n",
      "02:59:42 - epoch [536/2000], loss:2.93595943\n",
      "03:00:44 - epoch [541/2000], loss:9.54215049\n",
      "03:01:48 - epoch [546/2000], loss:2.92939786\n",
      "03:02:52 - epoch [551/2000], loss:7.81208515\n",
      "03:03:55 - epoch [556/2000], loss:2.92380933\n",
      "03:04:58 - epoch [561/2000], loss:9.05425164\n",
      "03:06:02 - epoch [566/2000], loss:2.91702470\n",
      "03:07:04 - epoch [571/2000], loss:7.72114185\n",
      "03:08:08 - epoch [576/2000], loss:2.91146418\n",
      "03:09:11 - epoch [581/2000], loss:7.70442980\n",
      "03:10:15 - epoch [586/2000], loss:2.90691826\n",
      "03:11:17 - epoch [591/2000], loss:7.66266123\n",
      "03:12:21 - epoch [596/2000], loss:2.90041889\n",
      "03:13:24 - epoch [601/2000], loss:7.71671633\n",
      "03:14:27 - epoch [606/2000], loss:2.89712329\n",
      "03:15:30 - epoch [611/2000], loss:7.59432364\n",
      "03:16:34 - epoch [616/2000], loss:2.88964162\n",
      "03:17:36 - epoch [621/2000], loss:7.55589588\n",
      "03:18:40 - epoch [626/2000], loss:2.88463553\n",
      "03:19:42 - epoch [631/2000], loss:7.78197633\n",
      "03:20:46 - epoch [636/2000], loss:2.87944455\n",
      "03:21:48 - epoch [641/2000], loss:7.95554268\n",
      "03:22:52 - epoch [646/2000], loss:2.87814965\n",
      "03:23:54 - epoch [651/2000], loss:7.46906371\n",
      "03:24:58 - epoch [656/2000], loss:2.87149927\n",
      "03:26:01 - epoch [661/2000], loss:8.27287492\n",
      "03:27:05 - epoch [666/2000], loss:2.86748209\n",
      "03:28:07 - epoch [671/2000], loss:7.75163721\n",
      "03:29:11 - epoch [676/2000], loss:2.86203520\n",
      "03:30:13 - epoch [681/2000], loss:7.59451574\n",
      "03:31:17 - epoch [686/2000], loss:3.02082970\n",
      "03:32:19 - epoch [691/2000], loss:8.67573441\n",
      "03:33:25 - epoch [696/2000], loss:2.85459674\n",
      "03:34:29 - epoch [701/2000], loss:7.37246664\n",
      "03:35:35 - epoch [706/2000], loss:2.85314299\n",
      "03:36:39 - epoch [711/2000], loss:7.42313091\n",
      "03:37:44 - epoch [716/2000], loss:2.92039794\n",
      "03:38:48 - epoch [721/2000], loss:7.51034974\n",
      "03:39:54 - epoch [726/2000], loss:2.92535865\n",
      "03:40:59 - epoch [731/2000], loss:7.76216262\n",
      "03:42:04 - epoch [736/2000], loss:2.94821707\n",
      "03:43:09 - epoch [741/2000], loss:8.02649184\n",
      "03:44:16 - epoch [746/2000], loss:2.83822699\n",
      "03:45:22 - epoch [751/2000], loss:7.37233712\n",
      "03:46:28 - epoch [756/2000], loss:3.00017322\n",
      "03:47:31 - epoch [761/2000], loss:7.26467893\n",
      "03:48:35 - epoch [766/2000], loss:2.83211318\n",
      "03:49:38 - epoch [771/2000], loss:7.54746792\n",
      "03:50:42 - epoch [776/2000], loss:2.82890850\n",
      "03:51:45 - epoch [781/2000], loss:7.27294250\n",
      "03:52:49 - epoch [786/2000], loss:3.00312121\n",
      "03:53:52 - epoch [791/2000], loss:7.74188744\n",
      "03:54:57 - epoch [796/2000], loss:2.91283515\n",
      "03:55:59 - epoch [801/2000], loss:7.22184160\n",
      "03:57:05 - epoch [806/2000], loss:2.82029663\n",
      "03:58:07 - epoch [811/2000], loss:8.07505680\n",
      "03:59:12 - epoch [816/2000], loss:2.81724086\n",
      "04:00:15 - epoch [821/2000], loss:7.32582368\n",
      "04:01:20 - epoch [826/2000], loss:2.81447342\n",
      "04:02:22 - epoch [831/2000], loss:7.15842938\n",
      "04:03:27 - epoch [836/2000], loss:2.93851308\n",
      "04:04:30 - epoch [841/2000], loss:7.13513070\n",
      "04:05:34 - epoch [846/2000], loss:2.80742724\n",
      "04:06:36 - epoch [851/2000], loss:7.18842102\n",
      "04:07:42 - epoch [856/2000], loss:2.80411440\n",
      "04:08:44 - epoch [861/2000], loss:7.42524971\n",
      "04:09:49 - epoch [866/2000], loss:2.80061757\n",
      "04:10:52 - epoch [871/2000], loss:7.08901499\n",
      "04:11:57 - epoch [876/2000], loss:2.79733287\n",
      "04:12:59 - epoch [881/2000], loss:7.25818803\n",
      "04:14:05 - epoch [886/2000], loss:2.92962338\n",
      "04:15:08 - epoch [891/2000], loss:7.05889796\n",
      "04:16:13 - epoch [896/2000], loss:2.94879721\n",
      "04:17:16 - epoch [901/2000], loss:7.04603853\n",
      "04:18:22 - epoch [906/2000], loss:2.93823350\n",
      "04:19:25 - epoch [911/2000], loss:7.04114044\n",
      "04:20:30 - epoch [916/2000], loss:2.78228571\n",
      "04:21:33 - epoch [921/2000], loss:7.01531545\n",
      "04:22:38 - epoch [926/2000], loss:2.77846538\n",
      "04:23:41 - epoch [931/2000], loss:7.02058943\n",
      "04:24:46 - epoch [936/2000], loss:2.77478299\n",
      "04:25:49 - epoch [941/2000], loss:6.98490451\n",
      "04:26:54 - epoch [946/2000], loss:2.77151264\n",
      "04:27:57 - epoch [951/2000], loss:6.97078178\n",
      "04:29:03 - epoch [956/2000], loss:2.76632052\n",
      "04:30:06 - epoch [961/2000], loss:7.29259454\n",
      "04:31:13 - epoch [966/2000], loss:2.77566402\n",
      "04:32:15 - epoch [971/2000], loss:6.93989985\n",
      "04:33:22 - epoch [976/2000], loss:2.75829538\n",
      "04:34:25 - epoch [981/2000], loss:6.94098913\n",
      "04:35:31 - epoch [986/2000], loss:2.75595158\n",
      "04:36:34 - epoch [991/2000], loss:6.93018275\n",
      "04:37:40 - epoch [996/2000], loss:2.75192253\n",
      "04:38:43 - epoch [1001/2000], loss:6.91586952\n",
      "04:39:49 - epoch [1006/2000], loss:2.74811116\n",
      "04:40:52 - epoch [1011/2000], loss:6.89057243\n",
      "04:41:58 - epoch [1016/2000], loss:2.74361593\n",
      "04:43:02 - epoch [1021/2000], loss:6.89046530\n",
      "04:44:08 - epoch [1026/2000], loss:2.74021715\n",
      "04:45:11 - epoch [1031/2000], loss:6.88243346\n",
      "04:46:18 - epoch [1036/2000], loss:2.73678051\n",
      "04:47:21 - epoch [1041/2000], loss:6.86877132\n",
      "04:48:28 - epoch [1046/2000], loss:2.73183863\n",
      "04:49:31 - epoch [1051/2000], loss:6.84814412\n",
      "04:50:37 - epoch [1056/2000], loss:2.72698769\n",
      "04:51:40 - epoch [1061/2000], loss:6.82513602\n",
      "04:52:47 - epoch [1066/2000], loss:2.72194719\n",
      "04:53:49 - epoch [1071/2000], loss:6.80904355\n",
      "04:54:56 - epoch [1076/2000], loss:2.71601002\n",
      "04:55:59 - epoch [1081/2000], loss:6.78379920\n",
      "04:57:05 - epoch [1086/2000], loss:2.71270311\n",
      "04:58:09 - epoch [1091/2000], loss:6.77575918\n",
      "04:59:16 - epoch [1096/2000], loss:2.79717156\n",
      "05:00:19 - epoch [1101/2000], loss:7.07456463\n",
      "05:01:27 - epoch [1106/2000], loss:2.85391506\n",
      "05:02:31 - epoch [1111/2000], loss:6.72525088\n",
      "05:03:37 - epoch [1116/2000], loss:2.76180597\n",
      "05:04:41 - epoch [1121/2000], loss:6.97561347\n",
      "05:05:47 - epoch [1126/2000], loss:2.68758391\n",
      "05:06:50 - epoch [1131/2000], loss:6.69132118\n",
      "05:07:56 - epoch [1136/2000], loss:2.68692364\n",
      "05:09:00 - epoch [1141/2000], loss:6.67966616\n",
      "05:10:07 - epoch [1146/2000], loss:2.78594776\n",
      "05:11:10 - epoch [1151/2000], loss:6.66091811\n",
      "05:12:17 - epoch [1156/2000], loss:2.67365654\n",
      "05:13:21 - epoch [1161/2000], loss:6.65526158\n",
      "05:14:27 - epoch [1166/2000], loss:2.67724467\n",
      "05:15:33 - epoch [1171/2000], loss:6.63505662\n",
      "05:16:41 - epoch [1176/2000], loss:2.66425795\n",
      "05:17:44 - epoch [1181/2000], loss:6.63042475\n",
      "05:18:52 - epoch [1186/2000], loss:2.67714302\n",
      "05:19:56 - epoch [1191/2000], loss:6.60937686\n",
      "05:21:03 - epoch [1196/2000], loss:2.65642295\n",
      "05:22:07 - epoch [1201/2000], loss:6.59494673\n",
      "05:23:13 - epoch [1206/2000], loss:2.65711647\n",
      "05:24:17 - epoch [1211/2000], loss:6.57220044\n",
      "05:25:25 - epoch [1216/2000], loss:2.68117160\n",
      "05:26:29 - epoch [1221/2000], loss:6.73147030\n",
      "05:27:36 - epoch [1226/2000], loss:2.69559682\n",
      "05:28:40 - epoch [1231/2000], loss:6.56429200\n",
      "05:29:47 - epoch [1236/2000], loss:2.64055480\n",
      "05:30:53 - epoch [1241/2000], loss:6.79798357\n",
      "05:32:01 - epoch [1246/2000], loss:2.63815094\n",
      "05:33:08 - epoch [1251/2000], loss:6.51309600\n",
      "05:34:17 - epoch [1256/2000], loss:2.65586899\n",
      "05:35:22 - epoch [1261/2000], loss:6.49990854\n",
      "05:36:29 - epoch [1266/2000], loss:2.63100161\n",
      "05:37:34 - epoch [1271/2000], loss:6.49092551\n",
      "05:38:41 - epoch [1276/2000], loss:2.66478746\n",
      "05:39:46 - epoch [1281/2000], loss:6.48367438\n",
      "05:40:54 - epoch [1286/2000], loss:2.67154930\n",
      "05:41:59 - epoch [1291/2000], loss:6.50808628\n",
      "05:43:07 - epoch [1296/2000], loss:2.62052587\n",
      "05:44:12 - epoch [1301/2000], loss:6.45858810\n",
      "05:45:20 - epoch [1306/2000], loss:2.61816229\n",
      "05:46:25 - epoch [1311/2000], loss:6.45748546\n",
      "05:47:33 - epoch [1316/2000], loss:2.61595548\n",
      "05:48:39 - epoch [1321/2000], loss:6.51323653\n",
      "05:49:47 - epoch [1326/2000], loss:2.61298072\n",
      "05:50:52 - epoch [1331/2000], loss:6.57805272\n",
      "05:51:59 - epoch [1336/2000], loss:2.61095901\n",
      "05:53:05 - epoch [1341/2000], loss:6.42597303\n",
      "05:54:13 - epoch [1346/2000], loss:2.60809936\n",
      "05:55:18 - epoch [1351/2000], loss:6.48545323\n",
      "05:56:26 - epoch [1356/2000], loss:2.65029509\n",
      "05:57:31 - epoch [1361/2000], loss:6.50021665\n",
      "05:58:39 - epoch [1366/2000], loss:2.72987422\n",
      "05:59:45 - epoch [1371/2000], loss:6.39664596\n",
      "06:00:53 - epoch [1376/2000], loss:2.62605539\n",
      "06:02:00 - epoch [1381/2000], loss:6.40297685\n",
      "06:03:08 - epoch [1386/2000], loss:2.59938527\n",
      "06:04:15 - epoch [1391/2000], loss:6.39181710\n",
      "06:05:22 - epoch [1396/2000], loss:2.59722543\n",
      "06:06:27 - epoch [1401/2000], loss:6.47302185\n",
      "06:07:35 - epoch [1406/2000], loss:2.59550432\n",
      "06:08:41 - epoch [1411/2000], loss:6.58276263\n",
      "06:09:49 - epoch [1416/2000], loss:2.59193614\n",
      "06:10:55 - epoch [1421/2000], loss:6.40040536\n",
      "06:12:03 - epoch [1426/2000], loss:2.59606515\n",
      "06:13:09 - epoch [1431/2000], loss:6.34555278\n",
      "06:14:17 - epoch [1436/2000], loss:2.58777521\n",
      "06:15:23 - epoch [1441/2000], loss:6.33602669\n",
      "06:16:31 - epoch [1446/2000], loss:2.58675011\n",
      "06:17:37 - epoch [1451/2000], loss:7.02042982\n",
      "06:18:45 - epoch [1456/2000], loss:2.58409925\n",
      "06:19:51 - epoch [1461/2000], loss:6.32069316\n",
      "06:20:59 - epoch [1466/2000], loss:2.58129046\n",
      "06:22:04 - epoch [1471/2000], loss:6.31076720\n",
      "06:23:12 - epoch [1476/2000], loss:2.58045548\n",
      "06:24:18 - epoch [1481/2000], loss:6.30368494\n",
      "06:25:26 - epoch [1486/2000], loss:2.57621202\n",
      "06:26:32 - epoch [1491/2000], loss:6.44078694\n",
      "06:27:40 - epoch [1496/2000], loss:2.57646420\n",
      "06:28:46 - epoch [1501/2000], loss:6.30041688\n",
      "06:29:54 - epoch [1506/2000], loss:2.57346116\n",
      "06:31:00 - epoch [1511/2000], loss:6.28743293\n",
      "06:32:08 - epoch [1516/2000], loss:2.57072892\n",
      "06:33:14 - epoch [1521/2000], loss:6.36927711\n",
      "06:34:22 - epoch [1526/2000], loss:2.56765860\n",
      "06:35:28 - epoch [1531/2000], loss:6.27960816\n",
      "06:36:36 - epoch [1536/2000], loss:2.56858156\n",
      "06:37:42 - epoch [1541/2000], loss:6.27127763\n",
      "06:38:50 - epoch [1546/2000], loss:2.56577940\n",
      "06:39:55 - epoch [1551/2000], loss:6.25354954\n",
      "06:41:03 - epoch [1556/2000], loss:2.66687111\n",
      "06:42:09 - epoch [1561/2000], loss:6.24693784\n",
      "06:43:17 - epoch [1566/2000], loss:2.62879099\n",
      "06:44:23 - epoch [1571/2000], loss:6.44202534\n",
      "06:45:31 - epoch [1576/2000], loss:2.55876140\n",
      "06:46:37 - epoch [1581/2000], loss:6.24057429\n",
      "06:47:44 - epoch [1586/2000], loss:2.55646304\n",
      "06:48:50 - epoch [1591/2000], loss:6.27682519\n",
      "06:49:58 - epoch [1596/2000], loss:2.55545183\n",
      "06:51:04 - epoch [1601/2000], loss:6.24604351\n",
      "06:52:11 - epoch [1606/2000], loss:2.55388089\n",
      "06:53:17 - epoch [1611/2000], loss:6.22473295\n",
      "06:54:25 - epoch [1616/2000], loss:2.55090455\n",
      "06:55:31 - epoch [1621/2000], loss:6.20151756\n",
      "06:56:39 - epoch [1626/2000], loss:2.54855945\n",
      "06:57:45 - epoch [1631/2000], loss:6.21452160\n",
      "06:58:52 - epoch [1636/2000], loss:2.54550278\n",
      "06:59:58 - epoch [1641/2000], loss:6.24330903\n",
      "07:01:05 - epoch [1646/2000], loss:2.54297694\n",
      "07:02:12 - epoch [1651/2000], loss:6.20506581\n",
      "07:03:20 - epoch [1656/2000], loss:2.54309354\n",
      "07:04:27 - epoch [1661/2000], loss:6.17478892\n",
      "07:05:34 - epoch [1666/2000], loss:2.53791014\n",
      "07:06:40 - epoch [1671/2000], loss:6.20383609\n",
      "07:07:48 - epoch [1676/2000], loss:2.61529983\n",
      "07:08:54 - epoch [1681/2000], loss:6.13870601\n",
      "07:10:01 - epoch [1686/2000], loss:2.53048459\n",
      "07:11:08 - epoch [1691/2000], loss:6.14761141\n",
      "07:12:16 - epoch [1696/2000], loss:2.55884959\n",
      "07:13:23 - epoch [1701/2000], loss:6.11829930\n",
      "07:14:30 - epoch [1706/2000], loss:2.61503320\n",
      "07:15:37 - epoch [1711/2000], loss:6.10361733\n",
      "07:16:44 - epoch [1716/2000], loss:2.56395445\n",
      "07:17:47 - epoch [1721/2000], loss:6.15540777\n",
      "07:18:53 - epoch [1726/2000], loss:2.60503211\n",
      "07:19:56 - epoch [1731/2000], loss:6.07012550\n",
      "07:21:02 - epoch [1736/2000], loss:2.55664147\n",
      "07:22:04 - epoch [1741/2000], loss:6.23915309\n",
      "07:23:10 - epoch [1746/2000], loss:2.50366325\n",
      "07:24:12 - epoch [1751/2000], loss:6.21306839\n",
      "07:25:18 - epoch [1756/2000], loss:2.50204132\n",
      "07:26:21 - epoch [1761/2000], loss:6.05098647\n",
      "07:27:27 - epoch [1766/2000], loss:2.49394780\n",
      "07:28:30 - epoch [1771/2000], loss:6.01259963\n",
      "07:29:36 - epoch [1776/2000], loss:2.56515572\n",
      "07:30:39 - epoch [1781/2000], loss:5.99260813\n",
      "07:31:46 - epoch [1786/2000], loss:2.56669455\n",
      "07:32:50 - epoch [1791/2000], loss:5.97769839\n",
      "07:33:58 - epoch [1796/2000], loss:2.47917925\n",
      "07:35:01 - epoch [1801/2000], loss:6.01564503\n",
      "07:36:08 - epoch [1806/2000], loss:2.49804284\n",
      "07:37:10 - epoch [1811/2000], loss:5.95285220\n",
      "07:38:17 - epoch [1816/2000], loss:2.55022539\n",
      "07:39:19 - epoch [1821/2000], loss:5.92525392\n",
      "07:40:26 - epoch [1826/2000], loss:2.54563929\n",
      "07:41:30 - epoch [1831/2000], loss:5.90528189\n",
      "07:42:36 - epoch [1836/2000], loss:2.48954724\n",
      "07:43:39 - epoch [1841/2000], loss:5.98602458\n",
      "07:44:46 - epoch [1846/2000], loss:2.44155720\n",
      "07:45:50 - epoch [1851/2000], loss:5.86136384\n",
      "07:46:57 - epoch [1856/2000], loss:2.43620758\n",
      "07:48:01 - epoch [1861/2000], loss:5.90321790\n",
      "07:49:08 - epoch [1866/2000], loss:2.51175871\n",
      "07:50:11 - epoch [1871/2000], loss:5.81134238\n",
      "07:51:18 - epoch [1876/2000], loss:2.42056628\n",
      "07:52:21 - epoch [1881/2000], loss:5.81666354\n",
      "07:53:28 - epoch [1886/2000], loss:2.42002947\n",
      "07:54:31 - epoch [1891/2000], loss:5.84709163\n",
      "07:55:38 - epoch [1896/2000], loss:2.40871721\n",
      "07:56:42 - epoch [1901/2000], loss:5.74182563\n",
      "07:57:49 - epoch [1906/2000], loss:2.40187076\n",
      "07:58:52 - epoch [1911/2000], loss:5.89896192\n",
      "07:59:59 - epoch [1916/2000], loss:2.39227492\n",
      "08:01:03 - epoch [1921/2000], loss:5.75064899\n",
      "08:02:10 - epoch [1926/2000], loss:2.46861843\n",
      "08:03:14 - epoch [1931/2000], loss:5.67390003\n",
      "08:04:21 - epoch [1936/2000], loss:2.36701065\n",
      "08:05:24 - epoch [1941/2000], loss:5.68707000\n",
      "08:06:30 - epoch [1946/2000], loss:2.43468875\n",
      "08:07:34 - epoch [1951/2000], loss:5.62285201\n",
      "08:08:41 - epoch [1956/2000], loss:2.40341149\n",
      "08:09:44 - epoch [1961/2000], loss:5.72751338\n",
      "08:10:51 - epoch [1966/2000], loss:2.37864631\n",
      "08:11:54 - epoch [1971/2000], loss:5.73099362\n",
      "08:13:01 - epoch [1976/2000], loss:2.33254663\n",
      "08:14:05 - epoch [1981/2000], loss:5.60198168\n",
      "08:15:12 - epoch [1986/2000], loss:2.34679062\n",
      "08:16:15 - epoch [1991/2000], loss:5.59180564\n",
      "08:17:22 - epoch [1996/2000], loss:2.31141825\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "reset_model()\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "train_model('cae_v5_lrelu_17')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
